{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avance de proyecto 3: Sistema de recomendación\n",
    "## Maestría en inteligencia artificial aplicada: Análisis de grandes volúmenes de datos\n",
    "\n",
    "\n",
    "Equipo:\n",
    "- Abraham Cabanzo Jimenez A01794355\n",
    "- Ignacio Antonio Ruiz Guerra A00889972\n",
    "- Moisés Díaz Malagón A01208580\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta entrega se condensan los 4 algoritmos presentados en las entregas 1 y 2:\n",
    "A fin de que pueda compararse su rendimiento y sus aplicaciones de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescribimos los algoritmos contenidos en las entregas 1 y 2, utilizando programación orientada a objetos y estandarizando entradas y salidas entre ellos para facilitar su uso y para facilitar la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import preprocessing_algo1\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import save_npz\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/2fb0wqk504s68c7h3cwcl1fc0000gn/T/ipykernel_93685/27823541.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings = pd.read_csv(\"ratings.dat\", sep='::', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
      "/var/folders/t3/2fb0wqk504s68c7h3cwcl1fc0000gn/T/ipykernel_93685/27823541.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  movies = pd.read_csv(\"movies.dat\", sep='::', names=['movie_id', 'movie_title', 'genres'], encoding='latin-1')\n"
     ]
    }
   ],
   "source": [
    "# BD para benchmark de primer algoritmo, basado en técnicas de NLP\n",
    "df = pd.read_csv(\"./imdb_top_1000.csv\")\n",
    "\n",
    "# BD para resto de experimentos presentados en las entregas anteriores\n",
    "ratings = pd.read_csv(\"ratings.dat\", sep='::', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "movies = pd.read_csv(\"movies.dat\", sep='::', names=['movie_id', 'movie_title', 'genres'], encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContentBasedNLP():\n",
    "    '''\n",
    "    Sistema de recomendación basado en contenido.\n",
    "    Técnicas de NLP, en específico TF-IDF y similaridad coseno.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, content):\n",
    "        nltk.download('stopwords')\n",
    "        self.stopwords = stopwords.words('english')\n",
    "        self.content = content\n",
    "        self.__generate_similarity_mtrx()\n",
    "        \n",
    "    def __generate_sentences(self, doc):\n",
    "        '''\n",
    "        El preprocesamiento a utilizar consiste en conservar únicamente caracteres alfabéticos, eliminando\n",
    "        signos de puntuación, caracteres especiales y números.\n",
    "        Solamente consideremos palabras (tokens) con longitud mayor a un caracter.\n",
    "        Convertiremos todo a minúsculas.\n",
    "        Eliminaremos del texto los stopwords del diccionario de stop words de la librería NLTK en inglés.\n",
    "        '''\n",
    "        words = re.sub(r'[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑ]', ' ', doc)\n",
    "        words = re.sub(r'\\s{2,}', ' ', words.strip())\n",
    "        words = words.lower().split()\n",
    "        tokens = [ w for w in words if w not in self.stopwords]\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def __generate_similarity_mtrx(self):\n",
    "        sentences = [self.__generate_sentences(x) for x in self.content['concatenated']]\n",
    "        tfidfvectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidfvectorizer.fit_transform(sentences)\n",
    "        self.similarity_mtrx = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "        self.movies_index = pd.Series(self.content.index, index=self.content['Series_Title']).drop_duplicates()\n",
    "\n",
    "    def recommend(self, movie_name):\n",
    "        idx = self.movies_index['Star Wars']\n",
    "        # obtenemos el vector de similitudes de esa pelicula con todas las otras\n",
    "        similarity_scores = list(enumerate(self.similarity_mtrx[idx]))\n",
    "        # ordenamos las pel[iculas con base en los scores de similaidad\n",
    "        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "        # podemos observar las 10 películas más similares, omitimos la primera pues es Star Wars\n",
    "        similarity_scores = similarity_scores[1:11]\n",
    "        title_idxs = [i[0] for i in similarity_scores]\n",
    "        return list(self.content['Series_Title'].iloc[title_idxs].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_recommender(top_n, movies):\n",
    "    '''\n",
    "    Recomendador global de películas.\n",
    "    Hace recomendaciones generales con base en la popularidad de las películas, las más vistas. En este caso se considerarán como las mas vistas, las más veces calificadas.\n",
    "    '''\n",
    "    movieid_to_title = dict(zip(movies['movie_id'], movies['movie_title']))\n",
    "    top_movies = ratings.value_counts('movie_id').sort_values(ascending=False).head(top_n)\n",
    "    titles = [movieid_to_title[x] for x in top_movies.tolist()]\n",
    "    movieids = top_movies.index.tolist()\n",
    "    return titles, movieids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilterBasic():\n",
    "    \"\"\"\n",
    "    Filtro colaborativo básico, basado en la matriz de usuario-item. \n",
    "    No toma en cuenta las características de las películas. \n",
    "    Funciona bajo la premisa de que a usuarios similares les interesan las mismas cosas.\n",
    "    Aprende de los intereses de la población. \n",
    "    \"\"\"\n",
    "    # matriz usuario-item, filas son usuarios, columnas son películas, contenido es la calificación bayesiana\n",
    "    # es una matriz dispersa\n",
    "\n",
    "    def __init__(self, movies, ratings):\n",
    "        self.movies = movies.copy()\n",
    "        self.ratings = ratings.copy()\n",
    "        self.__create_user_movie_matrix(self.ratings)\n",
    "        self.title_to_movieid = dict(zip(self.movies['movie_title'], list(self.movies['movie_id'])))\n",
    "        self.movieid_to_title = {v:k for k,v in self.title_to_movieid.items()}\n",
    "        self.model = None\n",
    "    \n",
    "    def find_movieid(self, title):\n",
    "        all_titles = self.movies['movie_title'].tolist()\n",
    "        closest_match = process.extractOne(title, all_titles)\n",
    "        return self.title_to_movieid[closest_match[0]]\n",
    "\n",
    "\n",
    "    def __create_user_movie_matrix(self, df):\n",
    "        unique_users = df['user_id'].unique()\n",
    "        unique_movies = df['movie_id'].unique()\n",
    "\n",
    "        self.userid_to_idx = dict(zip(unique_users, list(range(len(unique_users)))))\n",
    "        self.movieid_to_idx = dict(zip(unique_movies, list(range(len(unique_movies)))))\n",
    "\n",
    "        # mapeos inversos\n",
    "        self.idx_to_userid = {v:k for k,v in self.userid_to_idx.items()}\n",
    "        self.idx_to_movieid = {v:k for k,v in self.movieid_to_idx.items()}\n",
    "\n",
    "        users_idx = [self.userid_to_idx[id] for id in df['user_id']]\n",
    "        movies_idx = [self.movieid_to_idx[id] for id in df['movie_id']]\n",
    "\n",
    "        self.user_movie_matrix = csr_matrix((df['rating'], (users_idx, movies_idx)), shape=(len(unique_users), len(unique_movies)))\n",
    "\n",
    "        # persist to disk\n",
    "        save_npz('user_movie_matrix.npz', self.user_movie_matrix)\n",
    "\n",
    "    def train(self, k=10, metric='cosine'):\n",
    "        self.model = NearestNeighbors(n_neighbors=k, algorithm='brute', metric=metric)\n",
    "        self.model.fit(self.user_movie_matrix.T)\n",
    "\n",
    "        \n",
    "    def recommend(self, movie_title=None, movie_id=None, k=10):\n",
    "        if movie_id is None:\n",
    "            movie_id = self.find_movieid(movie_title)\n",
    "        distances, indices = self.model.kneighbors(self.user_movie_matrix.T[self.movieid_to_idx[movie_id]], n_neighbors=k+1)\n",
    "        distances = distances.squeeze()\n",
    "        indices = indices.squeeze()\n",
    "        movie_list = [self.movieid_to_title[self.idx_to_movieid[idx]] for idx in indices[1:]]\n",
    "        movies_ids = [self.idx_to_movieid[idx] for idx in indices[1:]]\n",
    "        \n",
    "        return movie_list, movies_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedBasic():\n",
    "    '''\n",
    "    Filtro basado en contenido. No toma en cuenta a los usuarios, sino las características de las películas, pero a diferencia del primer algoritmo, este no utiliza técnicas de NLP. \n",
    "\n",
    "    Soporta arranque en frio. \n",
    "    '''\n",
    "\n",
    "    def __init__(self, movies):\n",
    "        self.movies = movies.copy()\n",
    "        self.similarity_matrix = None\n",
    "        self.title_to_idx = dict(zip(self.movies['movie_title'], list(self.movies.index)))\n",
    "        self.title_to_movieid = dict(zip(self.movies['movie_title'], list(self.movies['movie_id'])))\n",
    "        self.movieid_to_idx = dict(zip(self.movies['movie_id'], list(self.movies.index)))\n",
    "        self.idx_to_title = {v:k for k,v in self.title_to_idx.items()}\n",
    "\n",
    "    def find_movie_idx(self, title):\n",
    "        all_titles = self.movies['movie_title'].tolist()\n",
    "        closest_match = process.extractOne(title, all_titles)\n",
    "        return self.title_to_idx[closest_match[0]]\n",
    "    \n",
    "    @staticmethod\n",
    "    def __calc_similarity_matrix(movies):\n",
    "        movies['genres'] = movies['genres'].apply(lambda x: x.split(\"|\"))\n",
    "        genre_counter = Counter(g for genres in movies['genres'] for g in genres)\n",
    "        # eliminamos las películas que no tienen género\n",
    "        movies = movies[movies['genres']!='(no genres listed)']\n",
    "        del genre_counter['(no genres listed)']\n",
    "\n",
    "        movies['clean_title'] = movies['movie_title'].apply(lambda x: re.sub(r'\\s*\\([^)]*\\)', '', x))\n",
    "        movies['year'] = movies['movie_title'].apply(ContentBasedBasic.__extract_year)\n",
    "        movies['decade'] = movies['year'].apply(lambda x: x - x%10)\n",
    "\n",
    "        # A continuación transformamos a one hot encoding los generos para generar un vector de 1s y 0s para cada película dependiendo si pertenece o no a un género en particular\n",
    "        genres = list(genre_counter.keys())\n",
    "        for gender in genres:\n",
    "            movies[gender] = movies['genres'].transform(lambda x: int(gender in x))\n",
    "\n",
    "        movie_decades = pd.get_dummies(movies['decade'])\n",
    "        movie_features = pd.concat([movies[genres], movie_decades], axis=1)\n",
    "\n",
    "        similarity_matrix = cosine_similarity(movie_features, movie_features)\n",
    "        return similarity_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def __extract_year(title):\n",
    "        year = re.search(r'\\((\\d{4})\\)', title)\n",
    "        return int(year.group(1)) if year else None\n",
    "\n",
    "    def train(self):\n",
    "        self.similarity_matrix = self.__calc_similarity_matrix(self.movies)\n",
    "\n",
    "\n",
    "    def recommend(self, movie_title=None, movie_id=None, top_n=10):\n",
    "        if movie_id:\n",
    "            movie_idx = self.movieid_to_idx[movie_id]\n",
    "        elif movie_title:\n",
    "            movie_idx = self.find_movie_idx(movie_title)\n",
    "        sim_scores = list(enumerate(self.similarity_matrix[movie_idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1:(top_n+1)]\n",
    "        similar_movies_idx = [i[0] for i in sim_scores]\n",
    "        similar_movies = [self.idx_to_title[x] for x in similar_movies_idx]\n",
    "        similar_moviesids = [self.title_to_movieid[x] for x in similar_movies]\n",
    "        return similar_movies, similar_moviesids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class RatingsDataset(Dataset):\n",
    "    '''\n",
    "    Dataset para generar los datos de entrenamiento para el modelo de deep learning.\n",
    "    Toma como entraada los ratings de las peliculas y transforma las calificaciones existentes en etiquetas de que el usuario vio la pelicula y genera N ejemplos no existentes para etiquetas de que el usuario no vio la pelicula. Esto servirá para entrenar el modelo de deep learning.\n",
    "\n",
    "    Args:\n",
    "        ratings: dataframe de pandas con las calificaciones\n",
    "        negative_examples_qty: cantidad de ejemplos negativos a generar\n",
    "        all_movieids: lista de IDs de todas las peliculas disponibles, no solo en set de entrenamiento\n",
    "    Returns:\n",
    "        tupla con id de usuario, id de pelicula y etiqueta\n",
    "\n",
    "    '''\n",
    "    def __init__(self, ratings, negative_examples_qty, all_movieids):\n",
    "        self.all_movieids = all_movieids\n",
    "        self.negative_examples_qty = negative_examples_qty\n",
    "        self.users, self.movies, self.labels = self._get_dataset(ratings, self.all_movieids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.movies[idx], self.labels[idx]\n",
    "\n",
    "    def _get_dataset(self, ratings, all_movieids):\n",
    "        users, movies, labels = [], [], []\n",
    "\n",
    "        # hacemos un set de pares, usuario - pelicula, no existen pares duplicados\n",
    "        user_movie_set = set(zip(ratings['user_id'], ratings['movie_id']))\n",
    "\n",
    "        for (user_id, movie_id) in user_movie_set:\n",
    "            users.append(user_id)\n",
    "            movies.append(movie_id)\n",
    "            labels.append(1) # consideramos que si calificó la película, la vio\n",
    "            for idx in range(self.negative_examples_qty):\n",
    "                # selecciona una pelicula aleatoria\n",
    "                negative_item = np.random.choice(all_movieids)\n",
    "                # revisar que el usuario no haya interactuado con esta pelicula antes\n",
    "                while (user_id, negative_item) in user_movie_set:\n",
    "                    negative_item = np.random.choice(all_movieids)\n",
    "                users.append(user_id)\n",
    "                movies.append(negative_item)\n",
    "                labels.append(0) # no vio la pelicula\n",
    "\n",
    "        return users, movies, labels\n",
    "\n",
    "class DLRecommenderModel(pl.LightningModule):\n",
    "    '''\n",
    "    Sistema de recomendcaión que utiliza un modelo de deep learning para implementar un filtro colaborativo que modela una función de similaridad entre vectores (embeddings) de usuarios y películas.\n",
    "\n",
    "    Args:\n",
    "        n_users: cantidad de usuarios\n",
    "        n_movies: cantidad de películas\n",
    "        ratings: base de datos de calificaiones de peliculas\n",
    "        all_movieids: lista de IDs de todas las peliculas disponibles, no solo en set de entrenamiento\n",
    "        negative_examples_qty: cantidad de ejemplos negativos a generar por cada positivo\n",
    "    '''\n",
    "\n",
    "    _EMBEDDING_DIMENSIONS = 6\n",
    "\n",
    "    def __init__(self, n_users, n_movies, ratings, all_movieids, negative_examples_qty):\n",
    "        super().__init__() # llamamos al constructor de la clase padre\n",
    "        self.negative_examples_qty = negative_examples_qty\n",
    "        self.user_embedding = nn.Embedding(num_embeddings=n_users, embedding_dim=self._EMBEDDING_DIMENSIONS) # usamos capa Embedding de pytorch\n",
    "        self.movie_embedding = nn.Embedding(num_embeddings=n_movies, embedding_dim=self._EMBEDDING_DIMENSIONS)\n",
    "\n",
    "        # posterior a las capas embeddings se aplican dos capas densas, también llamadas Full connected.\n",
    "        self.fc1 = nn.Linear(12, 64) # 12 features de entrada = 6 (de user embedding) + 6 (de movie embedding), 64 features de salida o neuronas\n",
    "        self.fc2 = nn.Linear(64, 32) # 64 features de entrada, 32 features de salida\n",
    "        self.fc3 = nn.Linear(32, 1) # 32 features de entrada, 1 feature de salida (prediccion)\n",
    "        self.ratings = ratings.copy()\n",
    "        self.all_movieids = all_movieids.copy()\n",
    "\n",
    "\n",
    "    def forward(self, user_id, movie_id):\n",
    "        # calculamos los embeddings\n",
    "        user_embedding = self.user_embedding(user_id)\n",
    "        movie_embedding = self.movie_embedding(movie_id)\n",
    "        # concatenamos los embeddings de usuario y pelicula\n",
    "        x = torch.cat([user_embedding, movie_embedding], dim=1)\n",
    "\n",
    "        # aplicamos las capas densas\n",
    "        x = nn.ReLU()(self.fc1(x)) # función de activación ReLU para capas intermedias\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        out = nn.Sigmoid()(self.fc3(x)) # función de activación sigmoide para obtener un valor entre 0 y 1 como salida final\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_ids, movie_ids, labels = batch\n",
    "        predictions = self(user_ids, movie_ids)\n",
    "        # utilizaremos la función de pérdida Binary Cross Entropy, pues nuestro problema de clasificación es de tipo binario (dos clases)\n",
    "        loss = nn.BCELoss()(predictions, labels.view(-1, 1).float())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # utilizamos Adam como optimizador, se puede ajustar el learning rate\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = RatingsDataset(self.ratings, self.negative_examples_qty, self.all_movieids)\n",
    "        return DataLoader(dataset, batch_size=512, num_workers=0)\n",
    "\n",
    "class CollaborativeFilterDL():\n",
    "    \"\"\"\n",
    "    Algoritmo avanzado. Filtro colaborativo basado en deep learning. \n",
    "    Utiliza retroalimentación implicita. Es decir, todas las peliculas que un usuario califica se consideran como vistas.\n",
    "    Mediante el entrenamiento se calculan los embeddings tanto de los usuarios como de las películas así como los pesos de las redes neuronales que modelan la función de similaridad entre los embeddings.  \n",
    "\n",
    "    Representa un problema de clasificación binaria, donde la etiqueta es 1 si el usuario vio la película y 0 si no la vio. \n",
    "\n",
    "    Requiere ser entrenado y actualizado. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratings, movies):\n",
    "        self.ratings = ratings.copy()\n",
    "        self.movies = movies.copy()\n",
    "        self.__prepare_data()\n",
    "        self.model = None\n",
    "        self.title_to_movieid = dict(zip(self.movies['movie_title'], list(self.movies['movie_id'])))\n",
    "        self.movieid_to_title = {v:k for k,v in self.title_to_movieid.items()}\n",
    "        \n",
    "    def find_movieid(self, title):\n",
    "        all_titles = self.movies['movie_title'].tolist()\n",
    "        closest_match = process.extractOne(title, all_titles)\n",
    "        return self.title_to_movieid[closest_match[0]]\n",
    "\n",
    "\n",
    "    def __prepare_data(self):\n",
    "        # utilizaremos los ultimos ejemplos de cada usuario como parte del test set\n",
    "        self.ratings['latest_rank'] = self.ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
    "        train_ratings = self.ratings[self.ratings['latest_rank'] != 1]\n",
    "        test_ratings = self.ratings[self.ratings['latest_rank'] == 1]\n",
    "\n",
    "        # conservamos unicamente user_id, movie_id, y rating\n",
    "        self.train_ratings = train_ratings[['user_id', 'movie_id', 'rating']]\n",
    "        self.test_ratings = test_ratings[['user_id', 'movie_id', 'rating']]\n",
    "\n",
    "        # Retroalimentación implícita: a todos los ratings se les considera como vistos por el usuario\n",
    "        self.train_ratings.loc[:, 'rating'] = 1 \n",
    "\n",
    "    def train(self):\n",
    "        num_users = self.ratings['user_id'].max() + 1\n",
    "        num_items = ratings['movie_id'].max() + 1\n",
    "        self.all_movieids = ratings['movie_id'].unique()\n",
    "\n",
    "        self.model = DLRecommenderModel(num_users, num_items, self.train_ratings, self.all_movieids, negative_examples_qty=5)\n",
    "\n",
    "        trainer = pl.Trainer(max_epochs=6, logger=False, accelerator='mps', devices=1)\n",
    "        trainer.fit(self.model)\n",
    "        return self.model\n",
    "\n",
    "    def recommend(self, user_id, model):\n",
    "        # movieid = self.find_movieid(movie_title)\n",
    "        not_seen_movies = self.ratings[self.ratings['user_id'] == user_id]['movie_id'].unique()\n",
    "        not_seen_scores = np.squeeze(model(torch.tensor([user_id]*len(not_seen_movies)), torch.tensor(not_seen_movies)).detach().numpy())\n",
    "\n",
    "        not_seen_pairs = zip(not_seen_movies, not_seen_scores)\n",
    "        not_seen_pairs = sorted(not_seen_pairs, key=lambda x: x[1], reverse=True)\n",
    "        top_recommended = not_seen_pairs[1:11]\n",
    "        top_recommended = [self.movieid_to_title[x[0]] for x in top_recommended]\n",
    "        return top_recommended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/moisesdiaz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "content_algo1 = preprocessing_algo1(df)\n",
    "content_nlp = ContentBasedNLP(content_algo1)\n",
    "\n",
    "collaborative_basic = CollaborativeFilterBasic(movies, ratings)\n",
    "\n",
    "content_basic = ContentBasedBasic(movies)\n",
    "\n",
    "collaborative_deeplearning = CollaborativeFilterDL(ratings, movies)\n",
    "\n",
    "# entrenamos los algoritmos, algunos son entrenamientos no supervisados y otros supervisados\n",
    "collaborative_basic.train()\n",
    "content_basic.train()\n",
    "trained_model = collaborative_deeplearning.train() # almacenamos el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de funcionamiento individual de cada algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Star Wars: Episode VI - Return of the Jedi',\n",
       " 'Star Wars: Episode V - The Empire Strikes Back',\n",
       " 'Lawrence of Arabia',\n",
       " 'The Bridge on the River Kwai',\n",
       " 'Star Wars: Episode VII - The Force Awakens',\n",
       " 'The Ladykillers',\n",
       " 'When Harry Met Sally...',\n",
       " 'Kind Hearts and Coronets',\n",
       " 'Raiders of the Lost Ark',\n",
       " 'The Fugitive']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_nlp.recommend('Star Wars') # sistema basado en contenido, usando técnicas de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Toy Story 2 (1999)',\n",
       "  'Groundhog Day (1993)',\n",
       "  'Aladdin (1992)',\n",
       "  \"Bug's Life, A (1998)\",\n",
       "  'Back to the Future (1985)',\n",
       "  'Babe (1995)',\n",
       "  'Star Wars: Episode V - The Empire Strikes Back (1980)',\n",
       "  'Men in Black (1997)',\n",
       "  'Forrest Gump (1994)',\n",
       "  'Matrix, The (1999)'],\n",
       " [3114, 1265, 588, 2355, 1270, 34, 1196, 1580, 356, 2571])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collaborative_basic.recommend(\"Toy Story\") # sistema de filtro colaborativo usando KNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hercules (1997)',\n",
       "  'Toy Story (1995)',\n",
       "  'Lion King, The (1994)',\n",
       "  'Nightmare Before Christmas, The (1993)',\n",
       "  'Beauty and the Beast (1991)',\n",
       "  'All Dogs Go to Heaven 2 (1996)',\n",
       "  'James and the Giant Peach (1996)',\n",
       "  'Hunchback of Notre Dame, The (1996)',\n",
       "  'Aladdin and the King of Thieves (1996)',\n",
       "  \"Cats Don't Dance (1997)\"],\n",
       " [1566, 1, 364, 551, 595, 631, 661, 783, 1064, 1489])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_basic.recommend('Aladin') # sistema basado en contenido usando one-hot y similaridad coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American Beauty (1999)',\n",
       " 'Forrest Gump (1994)',\n",
       " 'Star Wars: Episode VI - Return of the Jedi (1983)',\n",
       " 'Jurassic Park (1993)',\n",
       " 'Silence of the Lambs, The (1991)',\n",
       " 'Back to the Future (1985)',\n",
       " 'Braveheart (1995)',\n",
       " 'Star Wars: Episode I - The Phantom Menace (1999)',\n",
       " 'Matrix, The (1999)',\n",
       " 'Godfather, The (1972)']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user id demo\n",
    "user_id = 897\n",
    "collaborative_deeplearning.recommend(user_id, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizó la prueba de rendimiento a los algoritmos que se basan en el dataset de MovieLens, pues se requiere de un set de pruebas etiquetado para determinar si el algoritmo recomienda peliculas que en efecto el usuario ha visto. \n",
    "\n",
    "Se utilizará el métrico HitRatio@10 que indica la proporción de interacciones en las cuales el algoritmo ha recomendado al menos un elemento correcto (que el usuario ha visto) dentro del top 10 de elementos recomendados.\n",
    "\n",
    "Tomaremos el set de datos de prueba calculado para el filtro colaborativo basado en deep learning y las interacciones de la base de datos de ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = collaborative_deeplearning.test_ratings\n",
    "all_movieids = collaborative_deeplearning.all_movieids\n",
    "user_interactions = set(zip(test_set['user_id'], test_set['movie_id']))\n",
    "user_to_seen_movies_dict = ratings.groupby('user_id')['movie_id'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def calc_hit_ratio(model_name='collaborative_basic'):\n",
    "    hits = []\n",
    "    i = 0\n",
    "    for (user_id, movie_id) in tqdm(list(user_interactions)[0:100]):\n",
    "        seen_movies = user_to_seen_movies_dict[user_id]\n",
    "\n",
    "        if model_name == 'collaborative_deeplearning':\n",
    "            non_seen_movies = set(all_movieids) - set(seen_movies)\n",
    "            selected_movies = list(np.random.choice(list(non_seen_movies), 99))\n",
    "            test_movies = selected_movies + [movie_id]\n",
    "            predicted_labels = np.squeeze(trained_model(torch.tensor([user_id]*100), torch.tensor(test_movies)).detach().numpy())\n",
    "\n",
    "            top_10_idx = [test_movies[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()] # i puede ir de 0 a 100, convertimos a indice de pelicula\n",
    "        elif model_name == 'collaborative_basic':\n",
    "            for seen_movie in seen_movies:\n",
    "                # test_movie = np.random.choice(list(set(seen_movies) - set([movie_id])))\n",
    "                _, top_10_idx = collaborative_basic.recommend(movie_id=seen_movie)\n",
    "                if movie_id in top_10_idx:\n",
    "                    break\n",
    "            \n",
    "        elif model_name == 'content_basic':\n",
    "            for seen_movie in seen_movies:\n",
    "                # test_movie = np.random.choice(list(set(seen_movies) - set([movie_id])))\n",
    "                _, top_10_idx = content_basic.recommend(movie_id=seen_movie)\n",
    "                if movie_id in top_10_idx:\n",
    "                    break\n",
    "            # test_movie = np.random.choice(list(set(seen_movies) - set([movie_id])))\n",
    "            # _, top_10_idx = content_basic.recommend(collaborative_basic.movieid_to_title[test_movie])\n",
    "        elif model_name == 'global':\n",
    "            test_movie = np.random.choice(list(set(seen_movies) - set([movie_id])))\n",
    "            _, top_10_idx = global_recommender(top_n=10, movies=movies)\n",
    "\n",
    "\n",
    "        if movie_id in top_10_idx:\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "\n",
    "    return np.mean(hits)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe44f461122423a80b1ca225d0f7c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit ratio global: 0.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5f14e4b1d84766babf935c1918c096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit ratio collaborative_basic: 57.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e236b35d435f4f56ae5c95b972a5184a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit ratio content_basic: 52.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f80c33ef6f34736947463cacff67ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit ratio collaborative_deeplearning: 42.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hit ratio global: {calc_hit_ratio('global'):.2f}%\")\n",
    "print(f\"Hit ratio collaborative_basic: {calc_hit_ratio('collaborative_basic'):.2f}%\")\n",
    "print(f\"Hit ratio content_basic: {calc_hit_ratio('content_basic'):.2f}%\", )\n",
    "print(f\"Hit ratio collaborative_deeplearning: {calc_hit_ratio('collaborative_deeplearning'):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones que se pueden obtener de esta prueba:\n",
    "\n",
    "1. El recomendador global, basado en popularidad, es el que tiene menor tasa de aciertos, esto se debe a que recomienda las películas más populares, no necesariamente las que le gustan a un usuario en particular.\n",
    "2. El mejor rendimiento lo obtiene el filtro colaborativo basado en la interacción entre usuarios y películas usando Kneighbors, es rápido de entrenar y al ser un algoritmo no-supervisado no requiere de datos etiquetados. No requiere tampoco de metadatos de películas tal como descripciones o géneros. Por otro lado en tiempo de ejecución es el que tomó más tiempo de cómputo. Otro punto en contra es que este filtro será útil cuando el usuario ya tenga varias interacciones en la plataforma, no en un inicio. \n",
    "3. El segundo algoritmo con mejor rendimiento es el basado en contenido. Es de igual forma no supervisado y su matriz de similaridad se calcula rápidamente. Dado que se basa en el contenido de las películas y sus metadatos, no requiere que el usuario tenga interacciones previas, soporta arranque en frio. Por otro lado, otro beneficio es la velocidad de ejecución, al tener la matriz de similaridad pre-calculada la recomendacíon se genera en tiempo constante. \n",
    "4. Finalmente el 3er algoritmo con mejor rendimiento es el más complejo de los propuestos, basado en embeddings y redes neuronales. Este algoritmo genera dentro de sus pesos sinápticos una matriz de similaridad que combina usuarios y películas. Su complejidad y el poder de cómputo para su entrenamiento es considerablemente mayor que en el caso de los algoritmos anteriores. La inferencia sin embargo es rápida si se cuenta con hardware especializado como GPUs. En este caso logra un HitRatio@10 de 42%, el cual consideramos que podría incrementar si el algoritmo se entrena con más épocas, pues solo se entrenó en 4 épocas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
